import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from collections import defaultdict
import sys # Import sys for exiting on errors

def minimize_bead_distances(ds, npz_ds, csv_filepath, force_constant=10.0, learning_rate=0.001, num_steps=1000, device = 'cpu'):
    """
    Minimizes bead positions in a dataset (ds) to match target distances
    derived from a CSV file, using a harmonic potential and PyTorch optimization.

    Args:
        ds (dict): A dictionary containing bead information:
            - 'bead_residcs' (np.ndarray): 1D array of residue indices for each bead.
            - 'bead_segids' (np.ndarray): 1D array of segment IDs (strings) for each bead.
            - 'bead_names' (np.ndarray): 1D array of bead names (strings) for each bead.
            - 'bead_resnames' (np.ndarray): 1D array of residue names (strings) for each bead.
            - 'bead_pos' (np.ndarray): (num_frames, num_beads, 3) array of bead coordinates.
        csv_filepath (str): Path to the CSV file containing distance statistics
                            (generated by the previous script).
        force_constant (float): The force constant (k) for the harmonic potential.
                                A higher value means a stronger pull towards the target distance.
        learning_rate (float): Learning rate for the PyTorch optimizer (e.g., Adam).
        num_steps (int): Number of optimization steps to perform for each frame.

    Returns:
        np.ndarray: The updated 'bead_pos' array after minimization, shape (num_frames, num_beads, 3).
                    Returns the original 'bead_pos' if CSV reading fails or no connections are found.
    """
    print("Starting bead distance minimization...")

    # --- 1. Read CSV and Create Target Distances Dictionary ---
    print(f"Reading target distances from {csv_filepath}...")
    try:
        df = pd.read_csv(csv_filepath)
    except FileNotFoundError:
        print(f"Error: CSV file not found at {csv_filepath}")
        return ds['bead_pos'] # Return original positions if CSV is not found
    except Exception as e:
        print(f"An error occurred while reading the CSV file: {e}")
        return ds['bead_pos'] # Return original positions on other CSV errors

    # Create a dictionary mapping connection type to target distance (mean + 1 std)
    target_distances = {}
    target_tolerances = {}
    for index, row in df.iterrows():
        # Extract resname and beadname from the combined string
        resname1, beadname1 = row['resname1.beadname1'].split('.')
        resname2, beadname2 = row['resname2.beadname2'].split('.')

        # Create a consistent key for the connection type
        # For inter-residue BB-BB, sort resnames for the key to handle A-B and B-A consistently
        if beadname1 == 'BB' and beadname2 == 'BB' and resname1 != resname2:
             sorted_resnames = sorted([resname1, resname2])
             key = (sorted_resnames[0], 'BB', sorted_resnames[1], 'BB')
        else:
             # For intra-residue connections, use the names as they appear in the CSV
             # Assuming the CSV lists intra-residue connections in a consistent order (e.g., BB then SC1)
             key = (resname1, beadname1, resname2, beadname2)

        # Set the target distance as the mean + 1 standard deviation
        target_distances[key]  = row['mean_distance']
        target_tolerances[key] = row['std_distance']

    print(f"Loaded {len(target_distances)} unique target distances from CSV.")

    # --- 2. Identify Connections in the Input Data (ds) ---
    # We need to find which pairs of beads in the ds['bead_pos'] array correspond
    # to the connections defined by resname, beadname, resid, and segid.
    print("Identifying connections in the input dataset...")
    num_beads = ds['bead_pos'].shape[1]
    # List to store identified connections: (bead_index1, bead_index2, target_distance_key)
    connections_to_minimize = []

    # Define the order of relevant beads for identifying intra-residue connections
    # Get unique bead names from ds['bead_names'] and sort them for consistent order
    relevant_bead_names_order = sorted(np.unique(ds['bead_names']).tolist())
    # Create a mapping from bead name to its index in the ordered list
    bead_name_to_order_idx = {name: i for i, name in enumerate(relevant_bead_names_order)}

    # Iterate through all beads to find potential connections
    for i in range(num_beads):
        res_id1 = ds['bead_residcs'][i]
        seg_id1 = ds['bead_segids'][i]
        bead_name1 = ds['bead_names'][i]
        res_name1 = ds['bead_resnames'][i]

        # --- Intra-residue connections ---
        # Check for connections with beads in the *same* residue and segment
        for j in range(num_beads):
            if i == j: continue # Skip self-connection

            res_id2 = ds['bead_residcs'][j]
            seg_id2 = ds['bead_segids'][j]
            bead_name2 = ds['bead_names'][j]
            res_name2 = ds['bead_resnames'][j]

            # Check if beads are in the same residue and segment
            if res_id1 == res_id2 and seg_id1 == seg_id2:
                # Check if both bead names are in our relevant list and are consecutive in order
                if bead_name1 in bead_name_to_order_idx and bead_name2 in bead_name_to_order_idx:
                    idx1_order = bead_name_to_order_idx[bead_name1]
                    idx2_order = bead_name_to_order_idx[bead_name2]

                    # Check for consecutive order (e.g., BB -> SC1, SC1 -> SC2, SC1 -> SC4 etc.)
                    # We only add the connection once, ensuring idx1_order < idx2_order
                    if idx2_order > idx1_order:
                         # Key for intra-residue connection
                         key = (res_name1, bead_name1, res_name2, bead_name2)
                         # Check if this connection type has a target distance from the CSV
                         if key in target_distances:
                             # Store connection with bead indices and target distance key
                             connections_to_minimize.append((i, j, key))

        # --- Inter-residue connections (BB-BB) ---
        # Check for connections with beads in the *next* residue in the same segment
        for j in range(num_beads):
            if i == j: continue # Skip self-connection

            res_id2 = ds['bead_residcs'][j]
            seg_id2 = ds['bead_segids'][j]
            bead_name2 = ds['bead_names'][j]
            res_name2 = ds['bead_resnames'][j]

            # Check if beads are in consecutive residues and the same segment
            if res_id2 == res_id1 + 1 and seg_id1 == seg_id2:
                 # Check if both beads are BB
                 if bead_name1 == 'BB' and bead_name2 == 'BB':
                     # Key for inter-residue BB-BB connection (sorted bead_resnames)
                     sorted_resnames = sorted([res_name1, res_name2])
                     key = (sorted_resnames[0], 'BB', sorted_resnames[1], 'BB')
                     # Check if this connection type has a target distance from the CSV
                     if key in target_distances:
                          # Store connection with bead indices and target distance key
                          connections_to_minimize.append((i, j, key))

    # The above loop might add each connection twice (e.g., (i, j, key) and (j, i, key)).
    # We need to remove duplicates and ensure we only process each unique bond once.
    # We can use a set of tuples with sorted indices and the key.
    unique_connections_set = set()
    # List to store the final unique connections with original indices
    final_unique_connections = []

    for idx1, idx2, key in connections_to_minimize:
        # Create a unique representation of the connection by sorting the bead indices
        unique_rep = tuple(sorted((idx1, idx2))) + (key,)
        # Add to the set. If successful (it's a new unique connection), add to the list.
        if unique_rep not in unique_connections_set:
            unique_connections_set.add(unique_rep)
            final_unique_connections.append((idx1, idx2, key))

    print(f"Identified {len(final_unique_connections)} unique connections to minimize.")
    if not final_unique_connections:
        print("No relevant connections found in the dataset based on the CSV and bead types. Returning original positions.")
        return npz_ds

    # --- 3. Prepare PyTorch Tensors and Optimization ---
    # Convert positions to a PyTorch tensor. We will optimize frame by frame.
    original_pos = ds['bead_pos']
    num_frames = original_pos.shape[0]
    num_beads = original_pos.shape[1] # Get num_beads again for clarity

    # Create a NumPy array to store the minimized positions
    minimized_pos = np.copy(original_pos)

    # Initialize torch tensors
    original_pos = torch.tensor(original_pos, dtype=torch.float32, requires_grad=True, device=device)
    for k in target_distances.keys():
        target_distances[k]  = torch.tensor(target_distances[k] , device=device)
        target_tolerances[k] = torch.tensor(target_tolerances[k], device=device)
    
    idcs_1, idcs_2, masses_1, masses_2, targets, tolerances = [], [], [], [], [], []
    for idx1, idx2, key in final_unique_connections:
        idcs_1.append(idx1)
        idcs_2.append(idx2)
        _, bead_name1, _, bead_name2 = key
        masses_1.append(1.0 if bead_name1 == 'BB' else 0.1)
        masses_2.append(1.0 if bead_name2 == 'BB' else 0.1)
        targets.append(target_distances[key])
        tolerances.append(target_tolerances[key])
    idcs_1 = torch.tensor(idcs_1, device=device)
    idcs_2 = torch.tensor(idcs_2, device=device)
    masses_1 = torch.tensor(masses_1, device=device)
    masses_2 = torch.tensor(masses_2, device=device)
    targets = torch.tensor(targets, device=device)
    tolerances = torch.tensor(tolerances, device=device)

    print(f"Starting optimization for {num_frames} frames...")

    # Iterate through each frame for optimization
    for frame_idx in range(num_frames):
        print(f"Optimizing frame {frame_idx + 1}/{num_frames}...")

        # Get the positions for the current frame and convert to a PyTorch tensor
        # Set requires_grad=True because we want to compute gradients with respect to these positions
        current_frame_pos = original_pos[frame_idx]

        # --- Optimization Loop for the Current Frame ---
        for step in range(num_steps):

            # Allocate gradient buffer
            current_frame_pos.grad = torch.zeros_like(current_frame_pos)

            # Get bead positions
            pos1 = current_frame_pos[idcs_1]  # [E, 3]
            pos2 = current_frame_pos[idcs_2]  # [E, 3]

            # Compute displacement and distances
            disp = pos2 - pos1  # [E, 3]
            distances = torch.linalg.norm(disp, dim=-1, keepdim=True)  # [E, 1]
            directions = disp / (distances + 1e-8)  # [E, 3]

            # Compute displacement magnitude from target
            deviation = distances.squeeze(-1) - targets  # [E]
            clamped_dev = torch.clamp(torch.abs(deviation) - tolerances, min=0.0)  # [E]
            forces_magnitude = 2 * force_constant * clamped_dev  # derivative of the squared loss
            forces_magnitude = forces_magnitude.unsqueeze(-1)  # [E, 1]

            # Total force vector (direction * magnitude)
            force_vec = forces_magnitude * torch.sign(deviation).unsqueeze(-1) * directions  # [E, 3]

            # Compute mass-weighted splits
            inv_mass_sum = (1. / masses_1 + 1. / masses_2).unsqueeze(-1)  # [E, 1]
            weight1 = (1. / masses_1).unsqueeze(-1) / inv_mass_sum  # [E, 1]
            weight2 = (1. / masses_2).unsqueeze(-1) / inv_mass_sum  # [E, 1]

            # Apply mass-weighted gradients manually
            grad_pos1 = -weight1 * force_vec  # negative gradient
            grad_pos2 =  weight2 * force_vec  # positive gradient

            # Accumulate gradients (scattering manually)
            current_frame_pos.grad.index_add_(0, idcs_1, grad_pos1)
            current_frame_pos.grad.index_add_(0, idcs_2, grad_pos2)

            # Apply manual update
            with torch.no_grad():
                current_frame_pos -= learning_rate * current_frame_pos.grad

            # Optional: Print the loss periodically to monitor optimization progress
            # if (step + 1) % 50 == 0:
            #     print(f"  Frame {frame_idx + 1}, Step {step + 1}: Loss = {total_loss.item():.4f}")

        # After the optimization steps for the current frame are complete,
        # detach the tensor from the computation graph and convert it back to NumPy
        # to store in the results array.
        minimized_pos[frame_idx] = current_frame_pos.detach().numpy()
        
        print(f"Finished optimizing frame {frame_idx + 1}. Final Gradient Norm: {current_frame_pos.grad.norm().item():.4f}")

    npz_ds['bead_pos'] = minimized_pos
    print("Optimization process complete for all frames.")
    return npz_ds

# --- Example Usage (assuming you have a 'ds' dictionary and 'connected_bead_distance_stats.csv') ---
# # This is a commented-out example of how you might use the function.
# # You would replace this with your actual data loading and function call.

# # 1. Create a dummy ds dictionary (replace with your actual data loading)
# # Example structure:
# # num_frames_example = 2
# # num_beads_example = 10 # Example: 5 residues, each with BB and SC1 bead
# # ds_example = {
# #     'bead_residcs': np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5]), # Residue indices
# #     'bead_segids': np.array(['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A']), # Segment IDs
# #     'bead_names': np.array(['BB', 'SC1', 'BB', 'SC1', 'BB', 'SC1', 'BB', 'SC1', 'BB', 'SC1']), # Bead names
# #     'bead_resnames': np.array(['ALA', 'ALA', 'GLY', 'GLY', 'PRO', 'PRO', 'CYS', 'CYS', 'ALA', 'ALA']), # Residue names
# #     # Dummy positions - replace with your actual (num_frames, num_beads, 3) coordinates
# #     'bead_pos': np.random.rand(num_frames_example, num_beads_example, 3).astype(np.float32) * 10
# # }

# # 2. Ensure you have the CSV file generated by the previous script
# #    e.g., "connected_bead_distance_stats.csv"
# #    If you need a dummy CSV for testing, you can create one like this:
# # dummy_csv_data = {
# #     "resname1.beadname1": ["ALA.BB", "ALA.BB", "GLY.BB", "PRO.BB", "ALA.BB", "ALA.SC1", "GLY.BB", "PRO.BB"],
# #     "resname2.beadname2": ["GLY.BB", "ALA.SC1", "PRO.BB", "CYS.BB", "ALA.SC1", "ALA.SC2", "GLY.SC1", "PRO.SC1"],
# #     "count": [10, 20, 15, 12, 20, 5, 10, 10],
# #     "mean_distance": [0.5, 0.3, 0.55, 0.6, 0.3, 0.4, 0.35, 0.45],
# #     "std_distance": [0.05, 0.03, 0.06, 0.07, 0.03, 0.04, 0.035, 0.045],
# #     "min_distance": [0.4, 0.25, 0.45, 0.5, 0.25, 0.35, 0.3, 0.4],
# #     "max_distance": [0.6, 0.35, 0.65, 0.7, 0.35, 0.45, 0.4, 0.5],
# # }
# # dummy_df = pd.DataFrame(dummy_csv_data)
# # dummy_csv_filepath = "dummy_connected_bead_distance_stats.csv"
# # dummy_df.to_csv(dummy_csv_filepath, index=False)
# # print(f"Created dummy CSV file: {dummy_csv_filepath}")


# # 3. Call the minimization function
# # csv_file_path_to_use = "connected_bead_distance_stats.csv" # Replace with the actual path
# # minimized_positions = minimize_bead_distances(ds_example, csv_file_path_to_use, num_steps=200) # Adjust steps as needed

# # 4. The minimized positions are now in the 'minimized_positions' numpy array.
# #    You can replace the original 'bead_pos' in your ds dictionary if desired:
# # ds_example['bead_pos'] = minimized_positions

# # print("\nOriginal positions shape:", ds_example['bead_pos'].shape)
# # print("Minimized positions shape:", minimized_positions.shape)
# # print("\nExample of original positions (first frame, first 5 beads):\n", ds_example['bead_pos'][0, :5])
# # print("\nExample of minimized positions (first frame, first 5 beads):\n", minimized_positions[0, :5])

# # Optional: Clean up dummy file if created
# # import os
# # os.remove(dummy_csv_filepath)
# # print(f"\nRemoved dummy CSV file: {dummy_csv_filepath}")
