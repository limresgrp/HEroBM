# - general - #
root: results/neca
run_name: NECA.martini3
experiment_description: "NECA dataset using Martini-like CG."

seed: 42
dataset_seed: 42
append: true
default_dtype: float32

# --- M O D E L --- #

# -- network --
model_builders:
  # - herobm.backmapping.model.Model
  - HeadlessNodeModel
  - Heads
  - herobm.backmapping.model.HierarchicalReconstruction

# - cutoffs - #
r_max: 7.0

# - radial basis - #
edge_radial_attrs_basis: geqtrain.nn.BesselBasisVec
TanhCutoff_n: 6
num_basis: 8

# - symmetry - #
l_max: 3
parity: o3_full

# - general - #
avg_num_neighbors: 30
normalize_b2a_rel_vec: true
use_weight_norm: false

# - interaction layers - #
num_layers: 2
latent_dim: 32
env_embed_multiplicity: 8

two_body_latent_mlp_latent_dimensions: [128]
two_body_latent_mlp_nonlinearity: swiglu

latent_mlp_latent_dimensions: [128]
latent_mlp_nonlinearity: swiglu

env_embed_mlp_latent_dimensions: [128]
env_embed_mlp_nonlinearity: swiglu

# - attention - #
interaction_use_attention: false
# interaction_head_dim: 16

# - products - #
# use_mace_product: true

interaction_output_ls: [0, 1] # [Optional] used to avoid to compute l>1 as output of interaction, as I know I am not going to use l>1 later
# ---  END interaction layers  --- #
# --- START pooling layer --- #
edge_pooling_use_attention: false
# ---  END pooling layer  --- #

# --- START HEADS --- #
heads:
  - [node_output, 7x1o]

# --- head node_output --- #
head_node_output_readout_latent: geqtrain.nn.ScalarMLPFunction
head_node_output_mlp_latent_dimensions: [128]
head_mlp_nonlinearity: swiglu

# ---  END HEADS  --- #

# --- D A T A S E T --- #

# - train - #
dataset_list:
  - dataset: npz
    dataset_input: data/tutorial/NECA/npz
    key_mapping:
      bead_types: node_types
      bead_pos: pos
      bead2atom_rel_vectors: node_otuput
      bead2atom_reconstructed_idcs: bead2atom_reconstructed_idcs
      bead2atom_reconstructed_weights: bead2atom_reconstructed_weights
      lvl_idcs_mask: lvl_idcs_mask
      lvl_idcs_anchor_mask: lvl_idcs_anchor_mask
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
      atom_pos: atom_pos

# - register fields - #

node_fields:
  - node_output
  - bead2atom_reconstructed_idcs
  - bead2atom_reconstructed_weights
  - lvl_idcs_mask
  - lvl_idcs_anchor_mask

extra_fields:
  - atom_pos

type_names:
- NEC_AO2
- NEC_RO4
- NEC_RO3
- NEC_RO1
- NEC_Ar1
- NEC_Ar3
- NEC_Ar4
- NEC_Ar2
- NEC_RO0

# - define node attributes - #
node_attributes:
  node_types: # this kword must match the red kword in key_mapping
    embedding_dimensionality: 32
    fixed: true # if equal for each frame, if so they must not have the batch dim in the npz
  bead2atom_reconstructed_idcs:
    fixed: true
  bead2atom_reconstructed_weights:
    fixed: true
  lvl_idcs_mask:
    fixed: true
  lvl_idcs_anchor_mask:
    fixed: true

# - define extra attributes - #
extra_attributes:
  atom_bond_idx:
    fixed: true
  atom_angle_idx:
    fixed: true

target_names: ['atom_pos']
target_key: atom_pos

# --- L O S S --- #

loss_coeffs:
  - atom_pos:              # Loss on reconstructed atoms RMSD
    - .5
    - MSELoss
    - ignore_nan: true
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - .5
    - herobm.backmapping.train.InvariantsLoss
    - ignore_nan: true
      ignore_zeroes: true

# --- M E T R I C S --- #

metrics_components:
  - atom_pos: # Loss on reconstructed atoms RMSD
    - herobm.backmapping.train.RMSDLoss
    - ignore_nan: true

# --- L O G G I N G --- #

verbose: info
log_batch_freq: 100
wandb: false

# --- T R A I N I N G --- #

batch_size: 1
validation_batch_size: 1
dataloader_num_workers: 1

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM #
skip_chunking: true
# batch_max_atoms: 3000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

max_epochs: 10000
learning_rate: 1.0e-4
# noise: 0.05
train_val_split: random
shuffle: true
metrics_key: validation_loss

# - optimizer - #
optimizer_name: AdamW
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 1.e-5
# max_gradient_norm: 1.
# use_grokfast: true
sanitize_gradients: true
use_warmup: true
warmup_epochs: 5

# - scheduler - #
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 10
lr_scheduler_factor: 0.5

# - early stopping - #
early_stopping_lower_bounds:
  LR: 1.0e-7

early_stopping_patiences:
  validation_loss: 50