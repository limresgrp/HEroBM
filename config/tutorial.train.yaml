# - general - #
root: results/A2A
run_name: A2A.martini3
experiment_description: "A2A dataset using Martini 3.0 CG."

seed: 42
dataset_seed: 42
append: true
default_dtype: float32

# --- M O D E L --- #

# -- network --
model_builders:
  - HeadlessNodeModel
  - Heads
  - herobm.backmapping.model.HierarchicalReconstruction

# - cutoffs - #
r_max: 7.0

# - radial basis - #
edge_radial_attrs_basis: geqtrain.nn.BesselBasisVec
TanhCutoff_n: 6
num_basis: 8

# - symmetry - #
l_max: 3
parity: o3_full

# - general - #
# avg_num_neighbors: 30
normalize_b2a_rel_vec: true
use_weight_norm: false

# - interaction layers - #
num_layers: 3
latent_dim: 128
env_embed_multiplicity: 32

two_body_latent: geqtrain.nn.ScalarMLPFunction
two_body_latent_mlp_latent_dimensions: [512]
two_body_latent_mlp_nonlinearity: swiglu

latent: geqtrain.nn.ScalarMLPFunction
latent_mlp_latent_dimensions: [512]
latent_mlp_nonlinearity: swiglu

env_embed: geqtrain.nn.ScalarMLPFunction
env_embed_mlp_latent_dimensions: [512]
env_embed_mlp_nonlinearity: swiglu

# - attention - #
interaction_use_attention: true
interaction_head_dim: 32

# - products - #
# use_mace_product: true

interaction_output_ls: [0, 1] # [Optional] used to avoid to compute l>1 as output of interaction, as I know I am not going to use l>1 later
# ---  END interaction layers  --- #
# --- START pooling layer --- #
edge_pooling_use_attention: true
# ---  END pooling layer  --- #

# --- START HEADS --- #
heads:
  head_node_output:
    field: node_features
    out_field: node_output
    out_irreps: 5x1o

# --- head node_output --- #
head_node_output_readout_latent: geqtrain.nn.ScalarMLPFunction
head_node_output_mlp_latent_dimensions: [512]
head_mlp_nonlinearity: swiglu

# ---  END HEADS  --- #

# --- D A T A S E T --- #

# - train - #
dataset_list:
  - dataset: npz
    dataset_input: /home/angiod@usi.ch/HEroBM/tutorial/data/A2A/train # Folder containing all npz files to load as training dataset
    key_mapping:
      bead_types: node_types
      bead_pos: pos
      bead2atom_rel_vectors: node_otuput
      bead2atom_reconstructed_idcs: bead2atom_reconstructed_idcs
      bead2atom_reconstructed_weights: bead2atom_reconstructed_weights
      lvl_idcs_mask: lvl_idcs_mask
      lvl_idcs_anchor_mask: lvl_idcs_anchor_mask
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
      atom_pos: atom_pos

validation_dataset_list:
  - validation_dataset: npz
    validation_dataset_input: /home/angiod@usi.ch/HEroBM/tutorial/data/A2A/valid # Folder containing all npz files to load as validation dataset
    key_mapping:
      bead_types: node_types
      bead_pos: pos
      bead2atom_rel_vectors: node_otuput
      bead2atom_reconstructed_idcs: bead2atom_reconstructed_idcs
      bead2atom_reconstructed_weights: bead2atom_reconstructed_weights
      lvl_idcs_mask: lvl_idcs_mask
      lvl_idcs_anchor_mask: lvl_idcs_anchor_mask
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
      atom_pos: atom_pos

# - register fields - #

node_fields:
  - node_output
  - bead2atom_reconstructed_idcs
  - bead2atom_reconstructed_weights
  - lvl_idcs_mask
  - lvl_idcs_anchor_mask

extra_fields:
  - atom_pos

type_names: # Use the type names suggested when building the dataset
- ACE_RE
- BB
- ALA_SC1
- ARG_SC1
- ARG_SC2
- ASN_SC1
- ASP_SC1
- CYS_SC1
- CYX_SC1
- GLN_SC1
- GLU_SC1
- HSD_SC1
- HSD_SC2
- HSD_SC3
- HSE_SC1
- HSE_SC2
- HSE_SC3
- HIS_SC1
- HIS_SC2
- HIS_SC3
- ILE_SC1
- LEU_SC1
- LYS_SC1
- LYS_SC2
- MET_SC1
- NME_RE
- PHE_SC1
- PHE_SC2
- PHE_SC3
- PRO_SC1
- SER_SC1
- THR_SC1
- TRP_SC1
- TRP_SC2
- TRP_SC3
- TRP_SC4
- TRP_SC5
- TYR_SC1
- TYR_SC2
- TYR_SC3
- TYR_SC4
- VAL_SC1

# - define node attributes - #
node_attributes:
  node_types: # this kword must match the red kword in key_mapping
    embedding_dimensionality: 64
    fixed: true # if equal for each frame, if so they must not have the batch dim in the npz
  bead2atom_reconstructed_idcs:
    fixed: true
  bead2atom_reconstructed_weights:
    fixed: true
  lvl_idcs_mask:
    fixed: true
  lvl_idcs_anchor_mask:
    fixed: true

# - define extra attributes - #
extra_attributes:
  atom_bond_idx:
    fixed: true
  atom_angle_idx:
    fixed: true

target_names: ['atom_pos']
target_key: atom_pos

# --- L O S S --- #

loss_coeffs:
  - atom_pos:              # Loss on reconstructed atoms RMSD
    - .5
    - MSELoss
    - ignore_nan: true
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - .5
    - herobm.backmapping.train.InvariantsLoss
    - ignore_nan: true

# --- M E T R I C S --- #

metrics_components:
  - atom_pos: # Loss on reconstructed atoms RMSD
    - geqtrain.train.RMSDLoss
    - ignore_nan: true

# --- L O G G I N G --- #

verbose: info
log_batch_freq: 100
wandb: false

# --- T R A I N I N G --- #

batch_size: 1
validation_batch_size: 1
dataloader_num_workers: 2

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM #
skip_chunking: true   # If even 1 batch does not fit in you GPU, set this to false and modify 'batch_max_atoms' to make the chunked dataset fit
# batch_max_atoms: 3000 # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

max_epochs: 10000
learning_rate: 1.0e-4
# noise: 0.05
train_val_split: random
shuffle: true
metrics_key: validation_loss

# - optimizer - #
optimizer_name: AdamW
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 1.e-5
# max_gradient_norm: 1.
# use_grokfast: true
sanitize_gradients: true
use_warmup: true
warmup_epochs: 5

# - scheduler - #
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 10
lr_scheduler_factor: 0.5

# - early stopping - #
early_stopping_lower_bounds:
  LR: 1.0e-7

early_stopping_patiences:
  validation_loss: 50