# - general - #
root: /scratch/angiod/HEroBM/NECA/results/
run_name: NECA.martini3.Sep.2025
experiment_description: "NECA dataset using Martini-like CG."

seed: 42
dataset_seed: 42
append: true
default_dtype: float32

# --- M O D E L --- #

# -- network --
model_builders:
  - NodeModel
  - Heads
  - herobm.backmapping.model.HierarchicalReconstruction

# - general - #
use_weight_norm: false # Cannot jit compile when using this
strict_irreps: false
scalar_attnt: false
use_attention: true
use_mace_product: true
normalize_b2a_rel_vec: true
# - cutoffs - #
r_max: 6.0

# - radial basis - #
edge_radial_attrs_basis: geqtrain.nn.BesselBasisVec
TanhCutoff_n: 6
num_basis: 8

# - symmetry - #
l_max: 2
parity: so3

# - interaction layers - #
num_layers: 2
latent_dim: 128
env_embed_multiplicity: 8

latent: geqtrain.nn.ScalarMLPFunction
mlp_latent_dimensions: [256, 256]
mlp_nonlinearity: swiglu

# - attention - #
interaction_use_attention: false
# interaction_head_dim: 64

interaction_output_ls: [0, 1] # [Optional] used to avoid to compute l>1 as output of interaction, as I know I am not going to use l>1 later

# - pooling layer - #
pooling_use_attention: false

# --- START HEADS --- #
head_wds: 0.001
head_node_output_wd: true
heads:
  head_output:
    field: node_features
    out_field: node_output
    out_irreps: 7x1e
    model: geqtrain.nn.ReadoutModule

# ---  END HEADS  --- #

# --- D A T A S E T --- #

dataset_num_workers: 1

# - train - #
train_dataset_list:
  - dataset: npz
    dataset_input: data/tutorial/NECA/npz
    key_mapping:
      bead_types: node_types
      bead_pos: pos
      bead2atom_rel_vectors: node_otuput
      bead2atom_reconstructed_idcs: bead2atom_reconstructed_idcs
      bead2atom_reconstructed_weights: bead2atom_reconstructed_weights
      lvl_idcs_mask: lvl_idcs_mask
      lvl_idcs_anchor_mask: lvl_idcs_anchor_mask
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
      torsion_idcs: atom_dihedral_idx
      atom_pos: atom_pos

# - register fields - #

node_fields:
  - node_output
  - bead2atom_reconstructed_idcs
  - bead2atom_reconstructed_weights
  - lvl_idcs_mask
  - lvl_idcs_anchor_mask

extra_fields:
  - atom_pos

type_names:
- NEC_AO2
- NEC_RO4
- NEC_RO3
- NEC_RO1
- NEC_Ar1
- NEC_Ar3
- NEC_Ar4
- NEC_Ar2
- NEC_RO0

# - define node attributes - #
node_attributes:
  node_types: # this kword must match the red kword in key_mapping
    embedding_dimensionality: 8
    fixed: true # if equal for each frame, if so they must not have the batch dim in the npz
  bead2atom_reconstructed_idcs:
    fixed: true
  bead2atom_reconstructed_weights:
    fixed: true
  lvl_idcs_mask:
    fixed: true
  lvl_idcs_anchor_mask:
    fixed: true

# - define extra attributes - #
extra_attributes:
  atom_bond_idx:
    fixed: true
  atom_angle_idx:
    fixed: true
  atom_dihedral_idx:
    fixed: true

target_names: ['atom_pos']
target_key: atom_pos

# --- L O S S --- #

loss_coeffs:
  - atom_pos:              # Loss on reconstructed atoms RMSD
    - 0.8
    - MSELoss
    - ignore_nan: true
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - .2
    - herobm.backmapping.train.InvariantsLoss
    - ignore_nan: true
      ignore_zeroes: true

# --- M E T R I C S --- #

metrics_components:
  - atom_pos: # Loss on reconstructed atoms RMSD
    - RMSDMetric
    - ignore_nan: true

# --- L O G G I N G --- #

verbose: info
log_batch_freq: 100
wandb: false

# --- T R A I N I N G --- #

batch_size: 1
validation_batch_size: 1
dataloader_num_workers: 0

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM #
chunking: false
# batch_max_atoms: 3000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

max_epochs: 100
learning_rate: 1.0e-4
# noise: 0.05
train_val_split: random
shuffle: true
metrics_key: validation_loss

# - optimizer - #
optimizer_name: AdamW
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 1.e-5
max_gradient_norm: 10.
gradient_clipping_mode: fixed
# use_grokfast: true
sanitize_gradients: true
use_warmup: true
warmup_epochs: 3

# - scheduler - #
lr_scheduler_name: CosineAnnealingLR
eta_min: 1.e-7
# # drop lr if no improvement for tot epochs
# lr_scheduler_name: ReduceLROnPlateau
# lr_scheduler_patience: 10
# lr_scheduler_factor: 0.7

# - early stopping - #
early_stopping_lower_bounds:
  LR: 1.0e-7

early_stopping_patiences:
  validation_loss: 10